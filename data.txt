'Machine learning is a branch of artificial intelligence that enables algorithms to uncover hidden patterns within datasets, 
allowing them to make predictions on new, similar data without explicit programming for each task. Traditional machine learning 
combines data with statistical tools to predict outputs, yielding actionable insights. This technology finds applications in diverse
fields such as image and speech recognition, natural language processing, recommendation systems, fraud detection, portfolio optimization, 
and automating tasks. Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the "signal" or "feedback" available to the learning system:Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. [19]\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. [16] A representative book on research into machine learning during the 1960s was Nilsson\'s book on Learning Machines, dealing mostly with machine learning for pattern classification.\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users\' privacy to be maintained by not needing to send their data to a centralized server. Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. [33]\nThe difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). [134][135][136] Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration,[137][138] using approximate computing,[139] optimization of machine learning models and many more. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. [3][4]\nA subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams. [5][31] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. [127]\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. [64]\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. [55]\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. [65] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. [41]\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. [35]\nLeo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[29] wherein "algorithmic model" means more or less the machine learning algorithms like Random Forest. Machine learning (ML) is a field of inquiry devoted to understanding and building methods that "learn"\xa0– that is, methods that leverage data to improve performance on some set of tasks. Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[62][63] and finally meta-learning e.g. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. [109] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that "There\'s nothing artificial about AI...It\'s inspired by people, it\'s created by people, and—most importantly—it impacts people. [21]\nModern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. [2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. [78]\nTypically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. [100] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms. [6][7]\nSome implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. [61] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. [citation needed] Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer\'s part, no learning is needed. [84] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. [44]\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization. Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. [47]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \nIt is a system with only one input, situation, and only one output, action (or behavior) a. [42] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. [101][102] Machine learning systems used for criminal risk assessment have been found to be biased against black people. [76][77] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. Other approaches have been developed which don\'t fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. [114]\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories "spam" and well-visible "not spam" of posts) machine learning models which are often developed and/or trained by third parties. Various types of models have been used and researched for machine learning systems. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. [11]\nThe discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. [11]\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. [88] Recently, machine learning technology was also applied to optimize smartphone\'s performance and thermal behavior based on the user\'s interaction with the phone. [86] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. "[110]\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. [140][141]\nSoftware suites containing a variety of machine learning algorithms include the following: For example, Gboard uses federated machine learning to train search query prediction models on users\' mobile phones without having to send individual searches back to Google. Overfitting is something to watch out for when training a machine learning model. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. A machine learning algorithm for stock trading may inform the trader of future potential predictions. [96] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. [11]\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). [45]\nAs of 2022, deep learning is the dominant approach for much ongoing work in the field of machine learning. [83] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. [108] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. [8][9]\nIn its application across business problems, machine learning is also referred to as predictive analytics. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. When trained on man-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. [89][90][91]\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. [82] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). [22]\nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence. [40] Though unsupervised learning encompasses other domains involving summarizing and explaining data features. It involves computers learning from data provided so that they carry out certain tasks. [99]\nMachine learning approaches in particular can suffer from different data biases. [87] Machine learning was recently applied to predict the pro-environmental behavior of travelers. It contrasts with the "black box" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. [27] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.'


"As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[49] with two defining aspects:\nTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. [56] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[57] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[58] and new directions in artificial general intelligence based on the free energy principle[59] by British neuroscientist and theoretician at University College London Karl J. Friston. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[52] functional grammar,[53] construction grammar,[54] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[55] of the ACL). It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic i.e. [8]\nIn 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation. Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules."





CPython is distributed with a large standard library written in a mixture of C and native Python, and is available for many platforms, including Windows (starting with Python\xa03.9, the Python installer deliberately fails to install on Windows 7 and 8;[139][140] Windows XP was supported until Python\xa03.5) and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python\xa03.9.1, with experimental installer), with unofficial support for VMS. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming (including metaprogramming[69] and metaobjects). For example, the expression a < b < c tests whether a is less than b and b is less than c.[124] C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[125]\nPython uses arbitrary-precision arithmetic for all integer operations. Since 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where as of December\xa02022[update] it was the most popular language (ahead of C, C++, and Java). [218][219]\nPython has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modelers like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[220] Inkscape, Scribus and Paint Shop Pro,[221] and musical notation programs like scorewriter and capella. [209][210][211][212][213] As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing. [109] Python also provides methods, often called dunder methods (due to their names beginning and ending with double-underscores), to allow user-defined classes to modify how they are handled by native operations including length, comparison, in arithmetic operations and type conversion. [127]\nDue to Python\'s extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation. [68] Starting with 3.13, it and later versions have 2 years of full support (up from one and a half); followed by 3 years of security support (for same total support as before). [222] It has also been used in several video games,[223][224] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go. [40] It was selected as Programming Language of the Year (for "the highest rise in ratings in a year") in 2007, 2010, 2018, and 2020 (the only language to have done so four times as of 2020[update][193]). [171] Python\'s performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game. As well as standard desktop integrated development environments including PyCharm, IntelliJ Idea, Visual Studio Code etc, there are web browser-based IDEs, including SageMath, for developing science- and math-related programs; PythonAnywhere, a browser-based IDE and hosting environment; and Canopy IDE, a commercial IDE emphasizing scientific computing. Other just-in-time Python compilers have been developed, but are now unsupported:\nThere are several compilers/transpilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\nSpecialized:\nOlder projects (or not to be used with Python 3.x and latest syntax):\nPerformance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy \'13. [65]\nPython 3.12 adds syntax (and in fact every Python since at least 3.5 adds some syntax) to the language, the new (soft) keyword type (recent releases have added a lot of typing support e.g. Code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python. [208]\nPython is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, scikit-learn and the Logic language ProbLog. Van Rossum\'s vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which espoused the opposite approach. For example, the practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python, a PEP) is also used in Tcl,[243] Erlang,[244] and Swift. Python\'s design and philosophy have influenced many other programming languages:\nPython\'s development practices have also been emulated by other languages. [34][35]\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python\xa00.9.0. [85] Execution speed can be improved by moving speed-critical functions to extension modules written in languages such as C, or by using a just-in-time compiler like PyPy. Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora Linux use the Anaconda installer. [56]\nIn 2021 (and again twice in 2022), security updates were expedited, since all Python versions were insecure (including 2.7[57]) because of security issues leading to possible remote code execution[58] and web-cache poisoning. Typical applications of this combination include  natural language processing, visual query\nanswering, geospatial reasoning, and handling of semantic web data. [217]\nPython can also be used for graphical user interface (GUI) by using libraries like Tkinter. [46][47]\nPython 2.0 was released on 16 October 2000, with many major new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. SageMath is a computer algebra system with a notebook interface programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. [76]\nIts core philosophy is summarized in the Zen of Python (PEP 20), which includes aphorisms such as:[77]\nHowever, Python features regularly violate these principles and received criticism for adding unnecessary language bloat. program:\nProgram to calculate the factorial of a positive integer:\nPython\'s large standard library[130] provides tools suited to many tasks and is commonly cited as one of its greatest strengths. [186]\nPython 3.12 dropped some outdated modules, and more will be dropped in the future, deprecated as of 3.13; already deprecated array \'u\' format code will emit DeprecationWarning since 3.13 and will be removed in Python 3.16. [75] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML. Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, Ruby). Some standard library modules, 19 dead batteries, and many deprecated classes, functions and methods, and more will be removed in Python 3.15 and or 3.16. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Monty Python references appear frequently in Python code and culture;[188] for example, the metasyntactic variables often used in Python literature are spam and eggs instead of the traditional foo and bar. [215][216]\nThe Natlog system, implemented in Python, uses Definite Clause Grammars (DCGs) as prompt generators for text-to-text generators like GPT3 and text-to-image generators like DALL-E or Stable Diffusion. It is often described as a "batteries included" language due to its comprehensive standard library. As of 17\xa0March\xa02024,[update] the Python Package Index (PyPI), the official repository for third-party Python software, contains over 523,000[132] packages with a wide range of functionality, including:\nMost Python implementations (including CPython) include a read–eval–print loop (REPL), permitting them to function as a command line interpreter for which users enter statements sequentially and receive results immediately. [177]\nCPython\'s public releases come in three types, distinguished by which part of the version number is incremented:\nMany alpha, beta, and release-candidates are also released as previews and for testing before final releases. [119] These operators work like in traditional math; with the same precedence rules, the operators infix (+ and - can also be unary to represent positive and negative numbers respectively). Python 3.12 also drops outdated modules and functionality, and future versions will too, see below in Development section. [37]\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community. Before version\xa03.0, Python had two kinds of classes (both using the same syntax):  old-style and new-style;[111] current Python versions only support the semantics of the new style. [142] (During Python\xa01 and 2 development, even OS/2 and Solaris were supported,[143] but support has since been dropped for many platforms.) It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. This is reflected in its name—a tribute to the British comedy group Monty Python[87]—and in occasionally playful approaches to tutorials and reference materials, such as the use of the terms "spam" and "eggs" (a reference to a Monty Python sketch) in examples, instead of the often-used "foo" and "bar". It is also possible to cross-compile to other languages, but it either doesn\'t provide the full speed-up that might be expected, since Python is a very dynamic language, or a restricted subset of Python is compiled, and possibly semantics are slightly changed. "Pythonic" code may use Python idioms well, be natural or show fluency in the language, or conform with Python\'s minimalist philosophy and emphasis on readability. [173]\nEnhancement of the language corresponds with the development of the CPython reference implementation. [70] Many other paradigms are supported via extensions, including design by contract[71][72] and logic programming. Some other languages use indentation this way; but in most, indentation has no semantic meaning. Some parts of the standard library are covered by specifications—for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[131]—but most are specified by their code, internal documentation, and test suites. Python is used extensively in the information security industry, including in exploit development. [61] When Python\xa03.9.13 was released in May 2022, it was announced that the 3.9 series (joining the older series 3.8 and 3.7) would only receive security fixes in the future. Its design offers some support for functional programming in the Lisp tradition. [48] Python\xa03.0, released on 3 December 2008, with many of its major features backported to Python\xa02.6.x[49] and 2.7.x. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle, and Zope support developers in the design and maintenance of complex applications.'